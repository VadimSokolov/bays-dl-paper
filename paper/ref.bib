
@inproceedings{williams1997computing,
	title={Computing with infinite networks},
	author={Williams, Christopher KI},
	booktitle={Advances in neural information processing systems},
	pages={295--301},
	year={1997}
}
@article{polson2015statistical,
	title={A statistical theory of deep learning via proximal splitting},
	author={Polson, Nicholas G and Willard, Brandon T and Heidari, Massoud},
	journal={arXiv preprint arXiv:1509.06061},
	year={2015}
}
@incollection{neal1996priors,
	title={Priors for infinite networks},
	author={Neal, Radford M},
	booktitle={Bayesian Learning for Neural Networks},
	pages={29--53},
	year={1996},
	publisher={Springer}
}
@article{gramacy2011particle,
	title={Particle learning of Gaussian process models for sequential design and optimization},
	author={Gramacy, Robert B and Polson, Nicholas G},
	journal={Journal of Computational and Graphical Statistics},
	volume={20},
	number={1},
	pages={102--118},
	year={2011},
	publisher={Taylor \& Francis}
}
@article{banerjee2008gaussian,
	title={Gaussian predictive process models for large spatial data sets},
	author={Banerjee, Sudipto and Gelfand, Alan E and Finley, Andrew O and Sang, Huiyan},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={70},
	number={4},
	pages={825--848},
	year={2008},
	publisher={Wiley Online Library}
}
@article{lee2017deep,
	title={Deep Neural Networks as Gaussian Processes},
	author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	journal={arXiv preprint arXiv:1711.00165},
	year={2017}
}

@article{Gal2015Theoretically,
	Author = {Yarin Gal},
	Title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
	Year = {2015},
	Journal = {arXiv:1512.05287},
}
@misc{airbnb,
	title = {{Airbnb New User Bookings}},
	author= {Kaggle},
	year = {2015},
	howpublished = {\url{https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings}},
	note = {Accessed: 2017-09-11}
}


https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings
@inproceedings{louizos2016structured,
	title={Structured and efficient variational deep learning with matrix Gaussian posteriors},
	author={Louizos, Christos and Welling, Max},
	booktitle={International Conference on Machine Learning},
	pages={1708--1716},
	year={2016}
}
@article{mnih2014neural,
	title={Neural variational inference and learning in belief networks},
	author={Mnih, Andriy and Gregor, Karol},
	journal={arXiv preprint arXiv:1402.0030},
	year={2014}
}
@inproceedings{sutskever2014sequence,
	title={Sequence to sequence learning with neural networks},
	author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	booktitle={Advances in neural information processing systems},
	pages={3104--3112},
	year={2014}
}
@inproceedings{amodei2016deep,
	title={Deep speech 2: End-to-end speech recognition in english and mandarin},
	author={Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and others},
	booktitle={International Conference on Machine Learning},
	pages={173--182},
	year={2016}
}
@incollection{smolensky1986,
	address = {Cambridge, MA, USA},
	title = {Parallel {{Distributed Processing}}: {{Explorations}} in the {{Microstructure}} of {{Cognition}}, {{Vol}}. 1},
	isbn = {978-0-262-68053-0},
	shorttitle = {Parallel {{Distributed Processing}}},
	timestamp = {2017-08-21T01:58:46Z},
	publisher = {{MIT Press}},
	author = {Smolensky, P.},
	editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
	year = {1986},
	pages = {194--281}
}



@inproceedings{gal2016dropout,
	title={Dropout as a {Bayesian} approximation: Representing model uncertainty in deep learning},
	author={Gal, Yarin and Ghahramani, Zoubin},
	booktitle={international conference on machine learning},
	pages={1050--1059},
	year={2016}
}
@inproceedings{hernandez2015probabilistic,
	title={Probabilistic backpropagation for scalable learning of {Bayesian} neural networks},
	author={Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adams, Ryan},
	booktitle={International Conference on Machine Learning},
	pages={1861--1869},
	year={2015}
}
@article{blundell2015weight,
	title={Weight uncertainty in neural networks},
	author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	journal={arXiv preprint arXiv:1505.05424},
	year={2015}
}
@article{rezende2014stochastic,
	title={Stochastic backpropagation and approximate inference in deep generative models},
	author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	journal={arXiv preprint arXiv:1401.4082},
	year={2014}
}
@article{kingma2013auto,
	title={Auto-encoding variational {Bayes}},
	author={Kingma, Diederik P and Welling, Max},
	journal={arXiv preprint arXiv:1312.6114},
	year={2013}
}
@inproceedings{graves2011practical,
	title={Practical variational inference for neural networks},
	author={Graves, Alex},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2348--2356},
	year={2011}
}
@article{barber1998ensemble,
	title={Ensemble learning in {Bayesian} neural networks},
	author={Barber, David and Bishop, Christopher M},
	journal={Neural Networks and Machine Learning},
	volume={168},
	pages={215--238},
	year={1998},
	publisher={Springer Verlag}
}
@article{frey1999variational,
	title={Variational learning in nonlinear Gaussian belief networks},
	author={Frey, Brendan J and Hinton, Geoffrey E},
	journal={Neural Computation},
	volume={11},
	number={1},
	pages={193--213},
	year={1999},
	publisher={MIT Press}
}
@inproceedings{adams2010learning,
	title={Learning the structure of deep sparse graphical models},
	author={Adams, Ryan and Wallach, Hanna and Ghahramani, Zoubin},
	booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages={1--8},
	year={2010}
}
@article{lawrence2005probabilistic,
	title={Probabilistic non-linear principal component analysis with Gaussian process latent variable models},
	author={Lawrence, Neil},
	journal={Journal of machine learning research},
	volume={6},
	number={Nov},
	pages={1783--1816},
	year={2005}
}
@techreport{neal1992bayesian,
	title={{Bayesian} training of backpropagation networks by the hybrid Monte Carlo method},
	author={Neal, Radford M},
	year={1992},
	institution={Technical Report CRG-TR-92-1, Dept. of Computer Science, University of Toronto}
}
@article{neal1990learning,
	title={Learning stochastic feedforward networks},
	author={Neal, Radford M},
	journal={Department of Computer Science, University of Toronto},
	pages={64},
	year={1990}
}
@article{saul1996mean,
	title={Mean field theory for sigmoid belief networks},
	author={Saul, Lawrence K and Jaakkola, Tommi and Jordan, Michael I},
	journal={Journal of artificial intelligence research},
	volume={4},
	pages={61--76},
	year={1996}
}
@inproceedings{hinton1993keeping,
	title={Keeping the neural networks simple by minimizing the description length of the weights},
	author={Hinton, Geoffrey E and Van Camp, Drew},
	booktitle={Proceedings of the sixth annual conference on Computational learning theory},
	pages={5--13},
	year={1993},
	organization={ACM}
}
@inproceedings{ruiz2016generalized,
	title={The generalized reparameterization gradient},
	author={Ruiz, Francisco R and AUEB, Michalis Titsias RC and Blei, David},
	booktitle={Advances in Neural Information Processing Systems},
	pages={460--468},
	year={2016}
}
@article{mackay1992practical,
	title={A practical {Bayesian} framework for backpropagation networks},
	author={MacKay, David JC},
	journal={Neural computation},
	volume={4},
	number={3},
	pages={448--472},
	year={1992},
	publisher={MIT Press}
}
@inproceedings{salakhutdinov2009deep,
	title={Deep boltzmann machines},
	author={Salakhutdinov, Ruslan and Hinton, Geoffrey},
	booktitle={Artificial Intelligence and Statistics},
	pages={448--455},
	year={2009}
}
@inproceedings{welling2005exponential,
	title={Exponential family harmoniums with an application to information retrieval},
	author={Welling, Max and Rosen-Zvi, Michal and Hinton, Geoffrey E},
	booktitle={Advances in neural information processing systems},
	pages={1481--1488},
	year={2005}
}
@article{salakhutdinov2008learning,
	title={Learning and evaluating Boltzmann machines},
	author={Salakhutdinov, Ruslan},
	journal={Tech. Rep., Technical Report UTML TR 2008-002, Department of Computer Science, University of Toronto},
	year={2008}
}
@inproceedings{tieleman2008training,
	title={Training restricted Boltzmann machines using approximations to the likelihood gradient},
	author={Tieleman, Tijmen},
	booktitle={Proceedings of the 25th international conference on Machine learning},
	pages={1064--1071},
	year={2008},
	organization={ACM}
}
@inproceedings{hinton1983optimal,
	title={Optimal perceptual inference},
	author={Hinton, Geoffrey E and Sejnowski, Terrence J},
	booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
	pages={448--453},
	year={1983},
	organization={IEEE New York}
}
@article{poggio_deep_2016,
	title = {Deep {Learning}: {Mathematics} and {Neuroscience}},
	volume = {Brain-Inspired intelligent robotics: The intersection of robotics and neuroscience},
	shorttitle = {Deep {Learning}},
	journal = {A Sponsored Supplement to Science},
	author = {Poggio, T.},
	year = {2016},
	pages = {9--12},
	file = {Deep Learning\: Mathematics and Neuroscience | Poggio Lab:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/KGKGPMX7/deep-learning-mathematics-and-neuroscience-0.html:text/html}
}
@inproceedings{vitushkin1964some,
	title={Some properties of linear superpositions of smooth functions},
	author={Vitushkin, Anatoli Georgievich},
	booktitle={Dokl. Akad. Nauk SSSR},
	volume={156},
	pages={1003--1006},
	year={1964}
}
@article{poggio_why_2016,
	title = {Why and {When} {Can} {Deep} -- but {Not} {Shallow} -- {Networks} {Avoid} the {Curse} of {Dimensionality}: a {Review}},
	shorttitle = {Why and {When} {Can} {Deep} -- but {Not} {Shallow} -- {Networks} {Avoid} the {Curse} of {Dimensionality}},
	url = {http://arxiv.org/abs/1611.00740},
	abstract = {The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning. Deep convolutional networks are a special case of these conditions, though weight sharing is not the main reason for their exponential advantage.},
	urldate = {2017-05-27},
	journal = {arXiv:1611.00740 [cs]},
	author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.00740},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1611.00740 PDF:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/864CJS4Z/Poggio et al. - 2016 - Why and When Can Deep -- but Not Shallow -- Networ.pdf:application/pdf;arXiv.org Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/ERVC26GF/1611.html:text/html}
}
@article{ripley1994neural,
	title={Neural networks and related methods for classification},
	author={Ripley, Brian D},
	journal={Journal of the Royal Statistical Society. Series B (Methodological)},
	pages={409--456},
	year={1994},
	publisher={JSTOR}
}
@article{heaton2017deep,
	title={Deep learning for finance: deep portfolios},
	author={Heaton, JB and Polson, NG and Witte, Jan Hendrik},
	journal={Applied Stochastic Models in Business and Industry},
	volume={33},
	number={1},
	pages={3--12},
	year={2017},
	publisher={Wiley Online Library}
}
@misc{tensorflow2015-whitepaper,
	title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={http://tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
	Mart\'{\i}n~Abadi and
	Ashish~Agarwal and
	Paul~Barham and
	Eugene~Brevdo and
	Zhifeng~Chen and
	Craig~Citro and
	Greg~S.~Corrado and
	Andy~Davis and
	Jeffrey~Dean and
	Matthieu~Devin and
	Sanjay~Ghemawat and
	Ian~Goodfellow and
	Andrew~Harp and
	Geoffrey~Irving and
	Michael~Isard and
	Yangqing Jia and
	Rafal~Jozefowicz and
	Lukasz~Kaiser and
	Manjunath~Kudlur and
	Josh~Levenberg and
	Dan~Man\'{e} and
	Rajat~Monga and
	Sherry~Moore and
	Derek~Murray and
	Chris~Olah and
	Mike~Schuster and
	Jonathon~Shlens and
	Benoit~Steiner and
	Ilya~Sutskever and
	Kunal~Talwar and
	Paul~Tucker and
	Vincent~Vanhoucke and
	Vijay~Vasudevan and
	Fernanda~Vi\'{e}gas and
	Oriol~Vinyals and
	Pete~Warden and
	Martin~Wattenberg and
	Martin~Wicke and
	Yuan~Yu and
	Xiaoqiang~Zheng},
	year={2015},
}
@article{vitushkin_linear_1967,
	title = {LINEAR SUPERPOSITIONS OF FUNCTIONS},
	volume = {22},
	language = {en},
	number = {1},
	urldate = {2017-05-27},
	journal = {Russian Mathematical Surveys},
	author = {Vitushkin, A. G. and Khenkin, G. M.},
	year = {1967},
	pages = {77},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/B29IFPES/meta.html:text/html}
}
@article{VitHen67,
author ={ A.~G.~Vitushkin, G.~M.~Henkin},
title={Linear superpositions of functions},
journal = {Uspekhi Mat. Nauk},
year = {1967},
volume =  {22},
issue =  {1(133)},
pages =  {77--124},
volume={22}
}
@article{cook_fisher_2007,
	title = {Fisher {Lecture}: {Dimension} {Reduction} in {Regression}},
	volume = {22},
	shorttitle = {Fisher {Lecture}},
	abstract = {Beginning with a discussion of R. A. Fisher’s early written remarks that relate to dimension reduction, this article revisits principal components as a reductive method in regression, develops several model-based extensions and ends with descriptions of general approaches to model-based and model-free dimension reduction in regression. It is argued that the role for principal components and related methodology may be broader than previously seen and that the common practice of conditioning on observed values of the predictors may unnecessarily limit the choice of regression methodology.},
	language = {EN},
	number = {1},
	urldate = {2017-05-27},
	journal = {Statistical Science},
	author = {Cook, R. Dennis},
	month = feb,
	year = {2007},
	mrnumber = {MR2408655},
	zmnumber = {1246.62149},
	keywords = {Central subspace, Grassmann manifolds, inverse regression, minimum average variance estimation, principal components, principal fitted components, sliced inverse regression, sufficient dimension reduction},
	pages = {1--26},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/NK9JWQDC/1185975631.html:text/html}
}
@article{tran2016edward,
	author = {Dustin Tran and Alp Kucukelbir and Adji B. Dieng and Maja Rudolph and Dawen Liang and David M. Blei},
	title = {{Edward: A library for probabilistic modeling, inference, and criticism}},
	journal = {arXiv preprint arXiv:1610.09787},
	year = {2016}
}
@incollection{dean_large_2012,
	title = {Large {Scale} {Distributed} {Deep} {Networks}},
	urldate = {2017-05-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V. and Ng, Andrew Y.},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1223--1231},
	file = {NIPS Full Text PDF:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/EP6HN5HC/Dean et al. - 2012 - Large Scale Distributed Deep Networks.pdf:application/pdf;NIPS Snapshort:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/NAI6K64F/4687-large-scale-distributed-deep-networks.html:text/html}
}

@article{diaconis_nonlinear_1984,
	title = {On {Nonlinear} {Functions} of {Linear} {Combinations}},
	volume = {5},
	abstract = {Projection pursuit algorithms approximate a function of p variables by a sum of nonlinear functions of linear combinations: {\textbackslash}[ (1){\textbackslash}qquad f{\textbackslash}left( \{x\_1 , {\textbackslash}cdots ,x\_p \} {\textbackslash}right) {\textbackslash}doteq {\textbackslash}sum\_\{i = 1\}{\textasciicircum}n \{g\_i {\textbackslash}left( \{a\_\{i1\} x\_1  +  {\textbackslash}cdots  + a\_\{ip\} x\_p \} {\textbackslash}right)\} . {\textbackslash}] We develop some approximation theory, give a necessary and sufficient condition for equality in (1), and discuss nonuniqueness of the representation.},
	number = {1},
	urldate = {2017-05-27},
	journal = {SIAM Journal on Scientific and Statistical Computing},
	author = {Diaconis, P. and Shahshahani, M.},
	month = mar,
	year = {1984},
	pages = {175--191},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/DS4NNAZB/0905013.html:text/html}
}

@inproceedings{gallant_there_1988,
	title = {There exists a neural network that does not make avoidable mistakes},
	abstract = {The authors show that a multiple-input, single-output, single-hidden-layer feedforward network with (known) hardwired connections from input to hidden layer, monotone squashing at the hidden layer and no squashing at the output embeds as a special case a so-called Fourier network, which yields a Fourier series approximation properties of Fourier series representations. In particular, approximation to any desired accuracy of any square integrable function can be achieved by such a network, using sufficiently many hidden units. In this sense, such networks do not make avoidable mistakes.{\textless}{\textgreater}},
	booktitle = {{IEEE} 1988 {International} {Conference} on {Neural} {Networks}},
	author = {Gallant, A. R. and White, H.},
	month = jul,
	year = {1988},
	keywords = {avoidable mistakes, Fourier network, Fourier series approximation, monotone squashing, multiple input single output single hidden layer feedforward network, neural nets, Neural network, Neural networks},
	pages = {657--664 vol.1},
	file = {IEEE Xplore Abstract Record:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/GSUZJ283/23903.html:text/html}
}

@book{hastie_elements_2016,
	address = {New York, NY},
	edition = {2nd edition},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {978-0-387-84857-0},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	abstract = {This book describes the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of colour graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorisation, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates.},
	language = {English},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2016}
}
@article{wold1956causal,
	title={Causal inference from observational data: A review of end and means},
	author={Wold, Herman},
	journal={Journal of the Royal Statistical Society. Series A (General)},
	volume={119},
	number={1},
	pages={28--61},
	year={1956},
	publisher={JSTOR}
}
@article{hinton_reducing_2006,
	title = {Reducing the dimensionality of data with neural networks},
	volume = {313},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	language = {eng},
	number = {5786},
	journal = {Science (New York, N.Y.)},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pmid = {16873662},
	pages = {504--507}
}

@article{hinton_reducing_2006-1,
	title = {Reducing the dimensionality of data with neural networks},
	volume = {313},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	language = {eng},
	number = {5786},
	journal = {Science (New York, N.Y.)},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pmid = {16873662},
	pages = {504--507}
}

@article{kolmogorov_representation_1963,
	title = {On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition},
	volume = {28},
	number = {2},
	journal = {American Mathematical Society Translation},
	author = {Kolmogorov, Andrei Nikolaevich},
	year = {1963},
	pages = {55--59}
}
@article{sprecher_survey_1972,
	title = {A survey of solved and unsolved problems on superpositions of functions},
	volume = {6},
	number = {2},
	urldate = {2017-05-27},
	journal = {Journal of Approximation Theory},
	author = {Sprecher, David A.},
	year = {1972},
	pages = {123--134},
	file = {[HTML] sciencedirect.com:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/CNFCESX5/002190457290069X.html:text/html}
}
@book{bryant_analysis_2008,
	title = {Analysis of {Kolmogorov}'s superpostion theorem and its implementation in applications with low and high dimensional data},
	urldate = {2017-05-27},
	publisher = {University of Central Florida},
	author = {Bryant, Donald W.},
	year = {2008},
	file = {[PDF] ucf.edu:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/5BEXEFSU/Bryant - 2008 - Analysis of Kolmogorov's superpostion theorem and .pdf:application/pdf;Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/6FQ2RE4W/1.html:text/html}
}
@article{pascanu_how_2013,
	title = {How to {Construct} {Deep} {Recurrent} {Neural} {Networks}},
	abstract = {In this paper, we explore different ways to extend a recurrent neural network (RNN) to a {\textbackslash}textit\{deep\} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs.},
	urldate = {2017-05-27},
	journal = {arXiv:1312.6026 [cs, stat]},
	author = {Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6026},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICLR 2014 (Conference Track). 10-page text + 3-page references},
	file = {arXiv\:1312.6026 PDF:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/QPSBWTVM/Pascanu et al. - 2013 - How to Construct Deep Recurrent Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/EGQZZ5SW/1312.html:text/html}
}
@article{montufar_when_2015,
	title = {When {Does} a {Mixture} of {Products} {Contain} a {Product} of {Mixtures}?},
	volume = {29},
	number = {1},
	journal = {SIAM Journal on Discrete Mathematics},
	author = {Mont{\'u}far, Guido F. and Morton, Jason},
	year = {2015},
	pages = {321--347}
}
@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	number = {5},
	urldate = {2017-05-27},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
	file = {ScienceDirect Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/5S7XCGUZ/0893608089900208.html:text/html}
}

@article{hutchinson_nonparametric_1994,
	title = {A {Nonparametric} {Approach} to {Pricing} and {Hedging} {Derivative} {Securities} {Via} {Learning} {Networks}},
	volume = {49},
	abstract = {We propose a nonparametric method for estimating the pricing formula of a derivative asset using learning networks. Although not a substitute for the more traditional arbitrage-based pricing formulas, network-pricing formulas may be more accurate and computationally more efficient alternatives when the underlying asset's price dynamics are unknown, or when the pricing equation associated with the no-arbitrage condition cannot be solved analytically. To assess the potential value of network pricing formulas, we simulate Black-Scholes option prices and show that learning networks can recover the Black-Scholes formula from a two-year training set of daily options prices, and that the resulting network formula can be used successfully to both price and delta-hedge options out-of-sample. For comparison, we estimate models using four popular methods: ordinary least squares, radial basis function networks, multilayer perceptron networks, and projection pursuit. To illustrate the practical relevance of our network pricing approach, we apply it to the pricing and delta-hedging of S\&P 500 futures options from 1987 to 1991.},
	language = {en},
	number = {3},
	urldate = {2017-05-27},
	journal = {The Journal of Finance},
	author = {Hutchinson, James M. and Lo, Andrew W. and Poggio, Tomaso},
	month = jul,
	year = {1994},
	pages = {851--889},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/CAGI8IHK/abstract.html:text/html}
}

@incollection{lecun_efficient_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {BackProp}},
	copyright = {©1998 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-65311-0 978-3-540-49430-0},
	abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
	language = {en},
	number = {1524},
	urldate = {2017-05-27},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer Berlin Heidelberg},
	author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and Müller, Klaus-Robert},
	editor = {Orr, Genevieve B. and Müller, Klaus-Robert},
	year = {1998},
	keywords = {Artificial Intelligence (incl. Robotics), Complexity, Computation by Abstract Devices, Pattern Recognition, Processor Architectures},
	pages = {9--50},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/X8KU88ZK/10.html:text/html}
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	number = {6266},
	urldate = {2017-05-27},
	journal = {Science},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	year = {2015},
	pages = {1332--1338},
	file = {[PDF] uva.nl:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/E3UC8693/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf:application/pdf;Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/XI4DQN5E/1332.html:text/html}
}

@article{polson_proximal_2015,
	title = {Proximal algorithms in statistics and machine learning},
	volume = {30},
	number = {4},
	urldate = {2017-05-27},
	journal = {Statistical Science},
	author = {Polson, Nicholas G. and Scott, James G. and Willard, Brandon T. and {others}},
	year = {2015},
	pages = {559--581},
	file = {[PDF] arxiv.org:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/C9ET7JP7/Polson et al. - 2015 - Proximal algorithms in statistics and machine lear.pdf:application/pdf;Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/9PJJXBDI/1449670858.html:text/html}
}

@book{ripley_pattern_2007,
	title = {Pattern recognition and neural networks},
	urldate = {2017-05-27},
	publisher = {Cambridge university press},
	author = {Ripley, Brian D.},
	year = {2007},
	file = {[HTML] ox.ac.uk:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/PNKF3KS3/Compl.html:text/html;Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/DGF22PTS/books.html:text/html}
}

@article{sirignano_deep_2016,
	title = {Deep learning for limit order books},
	urldate = {2017-05-27},
	author = {Sirignano, Justin},
	year = {2016},
	file = {[PDF] arxiv.org:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/IS25RNPJ/Sirignano - 2016 - Deep learning for limit order books.pdf:application/pdf;Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/HRBCQGJF/papers.html:text/html}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting.},
	volume = {15},
	shorttitle = {Dropout},
	number = {1},
	urldate = {2017-05-27},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
	file = {[PDF] jmlr.org:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/6BJQ5WEZ/Srivastava et al. - 2014 - Dropout a simple way to prevent neural networks f.pdf:application/pdf}
}

@article{stein_estimation_1981,
	title = {Estimation of the mean of a multivariate normal distribution},
	urldate = {2017-05-27},
	journal = {The annals of Statistics},
	author = {Stein, Charles M.},
	year = {1981},
	pages = {1135--1151},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/QM9JVQQG/2240405.html:text/html}
}

@article{wold_causal_1956,
	title = {Causal inference from observational data: {A} review of end and means},
	volume = {119},
	shorttitle = {Causal inference from observational data},
	number = {1},
	urldate = {2017-05-27},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	author = {Wold, Herman},
	year = {1956},
	pages = {28--61},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/QVUFW2NB/2342961.html:text/html}
}
@article{aktekin2016sequential,
	title={Sequential {Bayesian} Analysis of Multivariate Count Data},
	author={Aktekin, Tevfik and Polson, Nicholas G and Soyer, Refik},
	journal={arXiv preprint arXiv:1602.01445},
	year={2016}
}

@article{polson_deep_2017,
	title = {Deep learning for short-term traffic flow prediction},
	volume = {79},
	abstract = {We develop a deep learning model to predict traffic flows. The main contribution is development of an architecture that combines a linear model that is fitted using ℓ 1 regularization and a sequence of tanh layers. The challenge of predicting traffic flows are the sharp nonlinearities due to transitions between free flow, breakdown, recovery and congestion. We show that deep learning architectures can capture these nonlinear spatio-temporal effects. The first layer identifies spatio-temporal relations among predictors and other layers model nonlinear relations. We illustrate our methodology on road sensor data from Interstate I-55 and predict traffic flows during two special events; a Chicago Bears football game and an extreme snowstorm event. Both cases have sharp traffic flow regime changes, occurring very suddenly, and we show how deep learning provides precise short term traffic flow predictions.},
	urldate = {2017-05-27},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Polson, Nicholas G. and Sokolov, Vadim O.},
	month = jun,
	year = {2017},
	keywords = {Deep Learning, Sparse linear models, Traffic Flows, Trend filtering},
	pages = {1--17}
}

@article{dixon2017,
	title = {Deep {Learning} for {Spatio}-{Temporal} {Modeling}: {Dynamic} {Traffic} {Flows} and {High} {Frequency} {Trading}},
	shorttitle = {Deep {Learning} for {Spatio}-{Temporal} {Modeling}},
	abstract = {Deep learning applies hierarchical layers of hidden variables to construct nonlinear high dimensional predictors. Our goal is to develop and train deep learning architectures for spatio-temporal modeling. Training a deep architecture is achieved by stochastic gradient descent (SGD) and drop-out (DO) for parameter regularization with a goal of minimizing out-of-sample predictive mean squared error. To illustrate our methodology, we predict the sharp discontinuities in traffic flow data, and secondly, we develop a classification rule to predict short-term futures market prices as a function of the order book depth. Finally, we conclude with directions for future research.},
	urldate = {2017-06-01},
	journal = {arXiv:1705.09851 [stat]},
	author = {Dixon, Matthew F. and Polson, Nicholas G. and Sokolov, Vadim O.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.09851},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1705.09851 PDF:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/TJ7HTKQI/Dixon et al. - 2017 - Deep Learning for Spatio-Temporal Modeling Dynami.pdf:application/pdf;arXiv.org Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/IMH29AA6/1705.html:text/html}
}


@article{carvalho2010particle,
	title={Particle learning for general mixtures},
	author={Carvalho, Carlos M and Lopes, Hedibert F and Polson, Nicholas G and Taddy, Matt A and others},
	journal={Bayesian Analysis},
	volume={5},
	number={4},
	pages={709--740},
	year={2010},
	publisher={International Society for Bayesian Analysis}
}
@inproceedings{snoek2012practical,
	title={Practical bayesian optimization of machine learning algorithms},
	author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	booktitle={Advances in neural information processing systems},
	pages={2951--2959},
	year={2012}
}
@article{johannes2009particle,
	title={Particle filtering},
	author={Johannes, Michael and Polson, Nicholas},
	journal={Handbook of Financial Time Series},
	pages={1015--1029},
	year={2009},
	publisher={Springer}
}
@article{johannes2009mcmc,
	title={Markov Chain Monte Carlo},
	author={Johannes, Michael and Polson, Nicholas},
	journal={Handbook of Financial Time Series},
	pages={1015-1029},
	year={2009},
	publisher={Springer}
}

@article{lopes2016particle,
	title={Particle learning for fat-tailed distributions},
	author={Lopes, Hedibert F and Polson, Nicholas G},
	journal={Econometric Reviews},
	volume={35},
	number={8-10},
	pages={1666--1691},
	year={2016},
	publisher={Taylor \& Francis}
}
@article{polson2008practical,
	title={Practical filtering with sequential parameter learning},
	author={Polson, Nicholas G and Stroud, Jonathan R and M{\"u}ller, Peter},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={70},
	number={2},
	pages={413--428},
	year={2008},
	publisher={Wiley Online Library}
}
@article{stroud2004practical,
	title={Practical filtering for stochastic volatility models},
	author={Stroud, Jonathan R and Polson, Nicholas G and M{\"u}ller, Peter},
	journal={State Space and U no b served Component Models (eds. A. Harvey, S. Koopmans and N. Shephard)},
	pages={236--247},
	year={2004}
}
@book{warty2014sequential,
	title={Sequential {Bayesian} learning for stochastic volatility with variance-gamma jumps in returns},
	author={Warty, Samir Pramod and Lopes, Hedibert F and Polson, Nicholas G},
	year={2014},
	publisher={University of Chicago, The University of Chicago Booth School of Business}
}
@article{heaton2016deep,
	title={Deep learning in finance},
	author={Heaton, JB and Polson, NG and Witte, JH},
	journal={arXiv preprint arXiv:1602.06561},
	year={2016}
}
@article{heaton2016deepa,
	title={Deep Portfolio Theory},
	author={Heaton, JB and Polson, NG and Witte, JH},
	journal={arXiv preprint arXiv:1605.07230},
	year={2016}
}
 @Manual{bass2017,
 	title = {BASS: {Bayesian} Adaptive Spline Surfaces},
 	author = {Devin Francom},
 	year = {2017},
 	note = {R package version 0.2.2},
 	url = {https://CRAN.R-project.org/package=BASS},
 }
@article{esteva2017dermatologist,
	title={Dermatologist-level classification of skin cancer with deep neural networks},
	author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
	journal={Nature},
	volume={542},
	number={7639},
	pages={115--118},
	year={2017},
	publisher={Nature Research}
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a rejoinder by the author)},
	volume = {16},
	shorttitle = {Statistical {Modeling}},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2017-03-04},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
	mrnumber = {MR1874152},
	zmnumber = {1059.62505},
	pages = {199--231},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/SA6SF9UQ/1009213726.html:text/html}
}
@misc{kubota_artificial_2017,
	title = {Artificial intelligence used to identify skin cancer},
	howpublished = {\url{http://news.stanford.edu/2017/01/25/artificial-intelligence-used-identify-skin-cancer/}},
	abstract = {In hopes of creating better access to medical care, Stanford researchers have trained an algorithm to diagnose skin cancer.},
	urldate = {2017-05-19},
	journal = {Stanford News},
	author = {Kubota, Taylor},
	month = jan,
	year = {2017},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/ITHPZ9PI/index2.html:text/html}
}
@misc{noauthor_deepmind_nodate,
	title = {{DeepMind} {AI} {Reduces} {Google} {Data} {Centre} {Cooling} {Bill} by 40\%},
	author = {DeepMind},
	year={2016},
	howpublished ={ \url{https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/}}
}

@misc{alphago,
	title = {{The story of AlphaGo so far}},
	author = {DeepMind},
	year={2017},
	howpublished ={ \url{https://deepmind.com/research/alphago/}}
}

@inproceedings{dean2012,
  title = {Large Scale Distributed Deep Networks},
	timestamp = {2017-02-17T18:25:24Z},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V. and {others}},
  urldate = {2017-02-17},
  year = {2012},
  pages = {1223--1231},
  file = {/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/DMK89SKC/Dean et al. - 2012 - Large scale distributed deep networks.pdf;/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/76DG5PHR/4687-large-scale-distributed-deep-networks.html}
}
@inproceedings{diaconis1987dozen,
	title={A dozen de Finetti-style results in search of a theory},
	author={Diaconis, Persi and Freedman, David},
	booktitle={Annales de l'IHP Probabilit{\'e}s et statistiques},
	volume={23},
	number={S2},
	pages={397--423},
	year={1987}
}
@inproceedings{nesterov1983method,
	title = {A method of solving a convex programming problem with convergence rate {O} (1/k2)},
	volume = {27},
	urldate = {2017-06-01},
	booktitle = {Soviet {Mathematics} {Doklady}},
	author = {Nesterov, Yurii},
	year = {1983},
	pages = {372--376},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/9XUS5C57/scholar.html:text/html}
}
@article{chipman2010bart,
	title={BART: {Bayesian} additive regression trees},
	author={Chipman, Hugh A and George, Edward I and McCulloch, Robert E and others},
	journal={The Annals of Applied Statistics},
	volume={4},
	number={1},
	pages={266--298},
	year={2010},
	publisher={Institute of Mathematical Statistics}
}
@phdthesis{gramacy2005bayesian,
	title={{Bayesian} treed Gaussian process models},
	author={Gramacy, Robert B},
	year={2005},
	school={University of California Santa Cruz}
}
@article{klartag2007central,
	title={A central limit theorem for convex sets},
	author={Klartag, Bo'az},
	journal={Inventiones mathematicae},
	volume={168},
	number={1},
	pages={91--131},
	year={2007},
	publisher={Springer}
}
@book{milman2009asymptotic,
	title={Asymptotic theory of finite dimensional normed spaces: Isoperimetric inequalities in riemannian manifolds},
	author={Milman, Vitali D and Schechtman, Gideon},
	volume={1200},
	year={2009},
	publisher={Springer}
}
@article{simonyan2014,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  timestamp = {2017-02-17T18:25:44Z},
  journaltitle = {arXiv preprint arXiv:1409.1556},
  author = {Simonyan, Karen and Zisserman, Andrew},
  urldate = {2017-02-17},
  year = {2014},
  file = {/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/K9JFQVD7/Simonyan and Zisserman - 2014 - Very deep convolutional networks for large-scale i.pdf;/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/K8REWEVF/1409.html}
}
@article{amit2000multiple,
	title={Multiple randomized classifiers: MRCL},
	author={Amit, Yali and Blanchard, Gilles and Wilder, Kenneth},
	year={2000},
	publisher={Citeseer}
}
@article{schmidhuber2015deep,
	title={Deep learning in neural networks: An overview},
	author={Schmidhuber, J{\"u}rgen},
	journal={Neural networks},
	volume={61},
	pages={85--117},
	year={2015},
	publisher={Elsevier}
}
@article{chenG16,
	author    = {Tianqi Chen and
	Carlos Guestrin},
	title     = {XGBoost: {A} Scalable Tree Boosting System},
	journal   = {CoRR},
	volume    = {abs/1603.02754},
	year      = {2016},
	timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChenG16},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{mallows1973some,
	title={Some comments on {C}p},
	author={Mallows, Colin L},
	journal={Technometrics},
	volume={15},
	number={4},
	pages={661--675},
	year={1973},
	publisher={Taylor \& Francis Group}
}
@book{lee_bayesian_2004,
	series = {{ASA}-{SIAM} {Series} on {Statistics} and {Applied} {Mathematics}},
	title = {Bayesian {Nonparametrics} via {Neural} {Networks}},
	isbn = {978-0-89871-563-7},
	abstract = {When I first heard about neural networks and how great they were, I was rather skeptical. Being sold as a magical black box, there was enough hype to make one believe that they could solve the world's problems. When I tried to learn more about them, I found that most of the literature was written for a machine learning audience, and I had to grapple with a new perspective and a new set of terminology. After some work, I came to see neural networks from a statistical perspective, as a probability model. One of the primary motivations for this book was to write about neural networks for statisticians, addressing issues and concerns of interest to statisticians, and using statistical terminology. Neural networks are a powerful model, and should be treated as such, rather than disdained as a “mere algorithm” as I have found some statisticians do. Hopefully this book will prove to be illuminating. The phrase “Bayesian nonparametrics” means different things to different people. The traditional interpretation usually implies infinite dimensional processes such as Dirichlet processes, used for problems in regression, classification, and density estimation. While on the surface this book may not appear to fit that description, it is actually close. One of the themes of this book is that a neural network can be viewed as a finite-dimensional approximation to an infinite-dimensional model, and that this model is useful in practice for problems in regression and classification. Thus the first section of this book will focus on introducing neural networks within the statistical context of nonparametric regression and classification. The rest of the book will examine important statistical modeling issues for Bayesian neural networks, particularly the choice of prior and the choice of model. While this book will not assume the reader has any prior knowledge about neural networks, neither will it try to be an all-inclusive introduction. Topics will be introduced in a self-contained manner, with references provided for further details of the many issues that will not be directly addressed in this book.},
	urldate = {2017-05-29},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Lee, H.},
	month = jan,
	year = {2004},
	file = {Snapshot:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/6SHTEAEM/1.html:text/html}
}
@article{zeiler2012adadelta,
	title={ADADELTA: an adaptive learning rate method},
	author={Zeiler, Matthew D},
	journal={arXiv preprint arXiv:1212.5701},
	year={2012}
}
@book{nesterov2013introductory,
	title={Introductory lectures on convex optimization: A basic course},
	author={Nesterov, Yurii},
	volume={87},
	year={2013},
	publisher={Springer Science \& Business Media}
}
@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}
@inproceedings{sutskever2013importance,
	title={On the importance of initialization and momentum in deep learning},
	author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	booktitle={International conference on machine learning},
	pages={1139--1147},
	year={2013}
}
@article{muller_issues_1998,
	title = {Issues in {Bayesian} {Analysis} of {Neural} {Network} {Models}},
	volume = {10},
	number = {3},
	urldate = {2017-05-29},
	journal = {Neural Computation},
	author = {M{\"u}ller, Peter and Insua, David Rios},
	month = apr,
	year = {1998},
	pages = {749--770},
}
@article{jiang2013sliced,
	title={Sliced inverse regression with variable selection and interaction detection},
	author={Jiang, Bo and Liu, Jun S},
	journal={arXiv preprint arXiv:1304.4056},
	volume={652},
	year={2013},
	publisher={Citeseer}
}
@article{gal2015dropout,
	title={Dropout as a {B}ayesian approximation: Representing model uncertainty in deep learning},
	author={Gal, Yarin and Ghahramani, Zoubin},
	journal={arXiv preprint arXiv:1506.02142},
	volume={2},
	year={2015}
}
@inproceedings{carreira2014distributed,
	title={Distributed optimization of deeply nested systems.},
	author={Carreira-Perpin{\'a}n, Miguel A and Wang, Weiran},
	booktitle={AISTATS},
	pages={10--19},
	year={2014}
}
@book{feller_introduction_1971,
	title = {An introduction to probability theory and its applications},
	isbn = {978-0-471-25709-7},
	abstract = {The exponential and the uniform densities; Special densities. Randomization; Densities in higher dimensions. Normal densities and processes; Probability measures and spaces; Probability distributions in Rr; A survey of some important distributions and processes; Laws of large numbers. Aplications in analysis; The basic limit theorems; Infinitely divisible distributions and semi-groups; Markov processes and semi-groups; Renewal theory; Random walks in R1; Laplace transforms. Tauberian theorems. Resolvents; Aplications of Laplace transforms; Characteristic functions; Expansions related to the central limit theorem; Infinitely divisible distributions; Applications of Fourier methods to ramdom walks; harmonic analysis; Answers to problems.},
	language = {en},
	publisher = {Wiley},
	author = {Feller, William},
	year = {1971},
	keywords = {Business \& Economics / Statistics, Fiction / General, Mathematics / General, Mathematics / Probability \& Statistics / General}
}
@article{sjoberg1995nonlinear,
	title={Nonlinear black-box modeling in system identification: a unified overview},
	author={Sj{\"o}berg, Jonas and Zhang, Qinghua and Ljung, Lennart and Benveniste, Albert and Delyon, Bernard and Glorennec, Pierre-Yves and Hjalmarsson, H{\aa}kan and Juditsky, Anatoli},
	journal={Automatica},
	volume={31},
	number={12},
	pages={1691--1724},
	year={1995},
	publisher={Elsevier}
}
@article{diaconis_nonlinear_1984,
	title = {On {Nonlinear} {Functions} of {Linear} {Combinations}},
	volume = {5},
	language = {en},
	number = {1},
	urldate = {2017-05-29},
	journal = {SIAM Journal on Scientific and Statistical Computing},
	author = {Diaconis, Persi and Shahshahani, Mehrdad},
	month = mar,
	year = {1984},
	pages = {175--191}
}
@article{diaconis1998consistency,
	title={Consistency of {Bayes} estimates for nonparametric regression: normal theory},
	author={Diaconis, Persi W and Freedman, David and others},
	journal={Bernoulli},
	volume={4},
	number={4},
	pages={411--444},
	year={1998},
	publisher={Bernoulli Society for Mathematical Statistics and Probability}
}
@article{frank1993statistical,
	title={A statistical view of some chemometrics regression tools},
	author={Frank, Ildiko E and Friedman, Jerome H},
	journal={Technometrics},
	volume={35},
	number={2},
	pages={109--135},
	year={1993},
	publisher={Taylor \& Francis Group}
}
@article{diaconis1981generating,
	title={Generating a random permutation with random transpositions},
	author={Diaconis, Persi and Shahshahani, Mehrdad},
	journal={Probability Theory and Related Fields},
	volume={57},
	number={2},
	pages={159--179},
	year={1981},
	publisher={Springer}
}
@article{neal1993bayesian,
	title={{Bayesian} learning via stochastic dynamics},
	author={Neal, Radford M},
	journal={Advances in neural information processing systems},
	pages={475--475},
	year={1993},
	publisher={Morgan Kaufmann Publishers}
}
@article{denker1987large,
	title={Large automatic learning, rule extraction, and generalization},
	author={Denker, John and Schwartz, Daniel and Wittner, Ben and Solla, Sara and Howard, Richard and Jackel, Lawrence and Hopfield, John},
	journal={Complex systems},
	volume={1},
	number={5},
	pages={877--922},
	year={1987}
}
@article{ackley1985learning,
	title={A learning algorithm for Boltzmann machines},
	author={Ackley, David H and Hinton, Geoffrey E and Sejnowski, Terrence J},
	journal={Cognitive science},
	volume={9},
	number={1},
	pages={147--169},
	year={1985},
	publisher={Wiley Online Library}
}
@article{amit_shape_1997,
	title = {Shape {Quantization} and {Recognition} with {Randomized} {Trees}},
	volume = {9},
	abstract = {We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures. No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions. The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred symbols. State-of-the-art error rates are achieved on the National- Institute of Standards and Technology database of digits. The principal goal of the experiments on symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context. Figure 1: LATEX Symbol},
	number = {7},
	journal = {Neural Computation},
	author = {Amit, Y. and Geman, D.},
	month = jul,
	year = {1997},
	pages = {1545--1588},
	file = {IEEE Xplore Abstract Record:/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/AJ8EDD2G/6795765.html:text/html}
}
@inproceedings{szegedy2015,
  title = {Going Deeper with Convolutions},
	timestamp = {2017-02-17T18:25:44Z},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  urldate = {2017-02-17},
  year = {2015},
  pages = {1--9},
  file = {/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/RVGBDEK4/Szegedy et al. - 2015 - Going deeper with convolutions.pdf;/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/UNKVNRTC/Szegedy_Going_Deeper_With_2015_CVPR_paper.html}
}



@article{friedman2001greedy,
	title = {Greedy Function Approximation: A Gradient Boosting Machine},
	timestamp = {2017-01-14T03:33:51Z},
	journaltitle = {Annals of statistics},
	author = {Friedman, Jerome H},
	date = {2001},
	pages = {1189--1232}
}

@article{li1991,
	title = {Sliced {{Inverse Regression}} for {{Dimension Reduction}}},
	volume = {86},
	abstract = {Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f(β 1 x, ..., β K x, ε), where the β k 's are the unknown row vectors. This model looks like a nonlinear regression, except for the crucial difference that the functional form of f is completely unknown. For effectively reducing the dimension, we need only to estimate the space [effective dimension reduction (e.d.r.) space] generated by the β k 's. This makes our goal different from the usual one in regression analysis, the estimation of all the regression coefficients. In fact, the β k 's themselves are not identifiable without a specific structural form on f. Our main theorem shows that under a suitable condition, if the distribution of x has been standardized to have the zero mean and the identity covariance, the inverse regression curve, E(x | y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix for the estimated inverse regression curve can be conducted to locate its main orientation, yielding our estimates for e.d.r. directions. Furthermore, we use a simple step function to estimate the inverse regression curve. No complicated smoothing is needed. SIR can be easily implemented on personal computers. By simulation, we demonstrate how SIR can effectively reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin-plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic is proposed to address the issue of whether or not a direction found by SIR is spurious.},
	timestamp = {2017-02-18T23:32:57Z},
	number = {414},
	journaltitle = {Journal of the American Statistical Association},
	author = {Li, Ker-Chau},
	urldate = {2017-02-18},
	date = {1991-06-01},
	year={1991},
	pages = {316--327},
	keywords = {Dynamic graphics,Principal component analysis,Projection pursuit},
	file = {/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/PFB2KQBH/01621459.1991.html}
}

@article{hartford2016,
	title = {Counterfactual {{Prediction}} with {{Deep Instrumental Variables Networks}}},
	abstract = {We are in the middle of a remarkable rise in the use and capability of artificial intelligence. Much of this growth has been fueled by the success of deep learning architectures: models that map from observables to outputs via multiple layers of latent representations. These deep learning algorithms are effective tools for unstructured prediction, and they can be combined in AI systems to solve complex automated reasoning problems. This paper provides a recipe for combining ML algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response. We show that a flexible IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework imposes some specific structure on the stochastic gradient descent routine used for training, but it is general enough that we can take advantage of off-the-shelf ML capabilities and avoid extensive algorithm customization. We outline how to obtain out-of-sample causal validation in order to avoid over-fit. We also introduce schemes for both Bayesian and frequentist inference: the former via a novel adaptation of dropout training, and the latter via a data splitting routine.},
	timestamp = {2017-02-18T23:34:56Z},
	archivePrefix = {arXiv},
	eprinttype = {arxiv},
	eprint = {1612.09596},
	primaryClass = {cs, stat},
	author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
	urldate = {2017-02-18},
	year={2017},
	date = {2016-12-30},
	keywords = {Computer Science - Learning,Statistics - Applications,Statistics - Machine Learning},
	file = {/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/W72I52HR/Hartford et al. - 2016 - Counterfactual Prediction with Deep Instrumental V.pdf;/Users/vsokolov/Library/Application Support/Zotero/Profiles/loshbpsk.default/zotero/storage/2UITZHM6/1612.html}
}

@article{wold2001,
	title={PLS-regression: a basic tool of chemometrics},
	author={Wold, Svante and Sj{\"o}str{\"o}m, Michael and Eriksson, Lennart},
	journal={Chemometrics and intelligent laboratory systems},
	volume={58},
	number={2},
	pages={109--130},
	year={2001},
	publisher={Elsevier}
}



